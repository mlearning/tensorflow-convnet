{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<H1 style=\"text-align: center;\"> TensorFlow Tutorial</H1>\n",
    "<H2 style=\"text-align: center;\"> Convolutional Neural Networks</H2>\n",
    "<footer style=\"text-align: center;\"> Kent Yu<br><br>10/14/2016</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "* **Problem** \n",
    "    * Classify RGB 32x32 pixel images across 10 categories\n",
    "    \n",
    "* **Solution**\n",
    "    * Build a relatively small convolutional neural network (CNN) for recognizing images\n",
    "    \n",
    "* **Implication**\n",
    "    * Provides a template for constructing larger and more sophisticated models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graph\n",
    "![tensorboard graph](./cifar_graph.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conv1 Layer\n",
    "* ** Input: 128 X 24 X 24 X 3 **\n",
    "* **Kernel size: 5 (Height) X 5 (Width) X 3 (Channels) X 64 (#of Kernels)**\n",
    "* **Activation: RELU **\n",
    "* ** Output: 128 X 24 X 24 X 64 **\n",
    "\n",
    "![tensorboard graph](./Conv1.jpg \"\")\n",
    "\n",
    "```python\n",
    "  # conv1\n",
    "  with tf.variable_scope('conv1') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights',\n",
    "                                         shape=[5, 5, 3, 64],\n",
    "                                         stddev=5e-2,\n",
    "                                         wd=0.0)\n",
    "    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(bias, name=scope.name)\n",
    "    _activation_summary(conv1)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Converlutional Network Illustration\n",
    "<img src=\"./convolutional-network-demo.gif\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pool 1 Layer\n",
    "* ** Input: 128 X 24 X 24 X 64 **\n",
    "\n",
    "* **KSize:  1 (Batch) X 3 (Height) X 3 (Width) X 1 (Channel) **\n",
    "* **Stride: 1 (Batch) X 2 (Height) X 2 (Width) X 1 (Channel) **\n",
    "* **Type: Max**\n",
    "* **Padding: Same**\n",
    "\n",
    "* ** Output: 128 X 12 X 12 X 64 **\n",
    "![tensorboard graph](./Pool1.jpg \"\")\n",
    "\n",
    "```Python\n",
    "  # pool1\n",
    "  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1],\n",
    "                         padding='SAME', name='pool1')\n",
    "```                         \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm1 Layer\n",
    "\n",
    "* tf.nn.lrn(**pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1'**)\n",
    "* **Accoring to CS231n, LRN is rarely used recently**\n",
    "> Many types of normalization layers have been proposed for use in ConvNet architectures,\n",
    "> ...\n",
    "> ...\n",
    "> However, these layers have since fallen out of favor because in practice their contribution has been shown to be minimal, if any. \n",
    "* What it does\n",
    "![tensorboard graph](./LRN.jpg \"\")\n",
    "\n",
    "```Python\n",
    "  # norm1\n",
    "  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm1')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2 Layer\n",
    "* ** Input: 128 X 12 X 12 X 64 **\n",
    "* **Kernel size: 5 (Height) X 5 (Width) X 64 (Channel) X 64 (# of Kernels)**\n",
    "* **Activation: RELU **\n",
    "* ** Output: 128 X 12 X 12 X 64 **\n",
    "![tensorboard graph](./Conv2.jpg \"\")\n",
    "\n",
    "```Python\n",
    " # conv2\n",
    "  with tf.variable_scope('conv2') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights',\n",
    "                                         shape=[5, 5, 64, 64],\n",
    "                                         stddev=5e-2,\n",
    "                                         wd=0.0)\n",
    "    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "    bias = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(bias, name=scope.name)\n",
    "    _activation_summary(conv2)\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm2 Layer\n",
    "\n",
    "* tf.nn.lrn(**conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,name='norm2')**)\n",
    "\n",
    "```Python\n",
    "  # norm2\n",
    "  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm2')\n",
    "```               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pool 2 Layer\n",
    "* ** Input: 128 X 12 X 12 X 64 **\n",
    "\n",
    "* **KSize:  1 (Batch) X 3 (Height) X 3 (Width) X 1 (Channel) **\n",
    "* **Stride: 1 (Batch) X 2 (Height) X 2 (Width) X 1 (Channel) **\n",
    "* ** Type: max **\n",
    "* **Padding: Same**\n",
    "\n",
    "* ** Output: 128 X 6 X 6 X 64 **\n",
    "![tensorboard graph](./Pool2.jpg \"\")\n",
    "\n",
    "```Python\n",
    "  # pool2\n",
    "  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "```                    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local 3 Layer\n",
    "\n",
    "* **Input: 128 * 6 * 6 * 64 **\n",
    "* ** Number of Nurons: 384**\n",
    "* ** Output: 128 * 384 **\n",
    "![tensorboard graph](./Local3.jpg \"\")\n",
    "\n",
    "```Python\n",
    "  # local3\n",
    "  with tf.variable_scope('local3') as scope:\n",
    "    # Move everything into depth so we can perform a single matrix multiply.\n",
    "    reshape = tf.reshape(pool2, [FLAGS.batch_size, -1])\n",
    "    dim = reshape.get_shape()[1].value\n",
    "    weights = _variable_with_weight_decay('weights', shape=[dim, 384],\n",
    "                                          stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local3)\n",
    "```    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local 4 Layer\n",
    "\n",
    "* ** Input: 128 * 384 **\n",
    "* ** Number of Nurons: 192**\n",
    "* ** Output: 128 * 192 **\n",
    "![tensorboard graph](./Local4.jpg \"\")\n",
    "```Python\n",
    "  # local4\n",
    "  with tf.variable_scope('local4') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', shape=[384, 192],\n",
    "                                          stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local4)\n",
    "```    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Softmax\n",
    "* **Input:128 * 192 **\n",
    "* **Number of classes: 10**\n",
    "* **Output: 10 (Probability)**\n",
    "![tensorboard graph](./Softmax.jpg \"\")\n",
    "```Python\n",
    "  # softmax, i.e. softmax(WX + b)\n",
    "  with tf.variable_scope('softmax_linear') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],\n",
    "                                          stddev=1/192.0, wd=0.0)\n",
    "    biases = _variable_on_cpu('biases', [NUM_CLASSES],\n",
    "                              tf.constant_initializer(0.0))\n",
    "    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "    _activation_summary(softmax_linear)\n",
    "```    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lost Function\n",
    "\n",
    "```Python\n",
    "def loss(logits, labels):\n",
    "  \"\"\"Add L2Loss to all the trainable variables.\n",
    "\n",
    "  Add summary for \"Loss\" and \"Loss/avg\".\n",
    "  Args:\n",
    "    logits: Logits from inference().\n",
    "    labels: Labels from distorted_inputs or inputs(). 1-D tensor\n",
    "            of shape [batch_size]\n",
    "\n",
    "  Returns:\n",
    "    Loss tensor of type float.\n",
    "  \"\"\"\n",
    "  # Calculate the average cross entropy loss across the batch.\n",
    "  labels = tf.cast(labels, tf.int64)\n",
    "  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      logits, labels, name='cross_entropy_per_example')\n",
    "  cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "  tf.add_to_collection('losses', cross_entropy_mean)\n",
    "\n",
    "  # The total loss is defined as the cross entropy loss plus all of the weight\n",
    "  # decay terms (L2 loss).\n",
    "  return tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "```  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reduce Mean\n",
    "\n",
    "reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None)\n",
    "input_tensor: The tensor to reduce. Should have numeric type.\n",
    "reduction_indices: The dimensions to reduce. If `None` (the defaut),\n",
    "    reduces all dimensions.\n",
    "```Python\n",
    "# 'x' is [[1., 1. ]]\n",
    "#         [2., 2.]]\n",
    "tf.reduce_mean(x) ==> 1.5\n",
    "tf.reduce_mean(x, 0) ==> [1.5, 1.5]\n",
    "tf.reduce_mean(x, 1) ==> [1.,  2.]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat_softmax= [[ 0.227863    0.61939586  0.15274114]\n",
      " [ 0.49674623  0.20196195  0.30129182]]\n",
      "loss_per_instance_1= [ 0.4790107   1.19967598]\n",
      "total_loss_1= 0.839343338979\n",
      "loss_per_instance_2= [ 0.4790107   1.19967598]\n",
      "loss_total_2= 0.839343338979\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('Rank mismatch: Labels rank (received %s) should equal logits rank (received %s) - 1.', 2, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-605bd01bba18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtotal_loss_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"loss_total_3=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/i809765/Source/Python/virtualenv/TensorFlowPython35/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[0;34m(logits, labels, name)\u001b[0m\n\u001b[1;32m    558\u001b[0m       raise ValueError(\"Rank mismatch: Labels rank (received %s) should equal \"\n\u001b[1;32m    559\u001b[0m                        \u001b[0;34m\"logits rank (received %s) - 1.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m                        labels_static_shape.ndims, logits.get_shape().ndims)\n\u001b[0m\u001b[1;32m    561\u001b[0m     \u001b[0;31m# Check if no reshapes are required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Rank mismatch: Labels rank (received %s) should equal logits rank (received %s) - 1.', 2, 2)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Assume y_hat is the calculated result from the tensor graph (Logit)\n",
    "y_hat = tf.convert_to_tensor(np.array([[0.5, 1.5, 0.1],[2.2, 1.3, 1.7]]))\n",
    "#print (\"y_hat=\",sess.run(y_hat))\n",
    "\n",
    "#Softmax:\n",
    "# For each batch `i` and class `j` we have\n",
    "#      softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))\n",
    "y_hat_softmax = tf.nn.softmax(y_hat)\n",
    "print (\"y_hat_softmax=\",sess.run(y_hat_softmax))\n",
    "\n",
    "y_true = tf.convert_to_tensor(np.array([[0.0, 1.0, 0.0],[0.0, 0.0, 1.0]]))\n",
    "#print (\"y_true=\", sess.run(y_true))\n",
    "\n",
    "loss_per_instance_1 = -tf.reduce_sum(y_true * tf.log(y_hat_softmax), reduction_indices=[1])\n",
    "print (\"loss_per_instance_1=\",sess.run(loss_per_instance_1))\n",
    "\n",
    "total_loss_1 = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_hat_softmax), reduction_indices=[1]))\n",
    "print (\"total_loss_1=\",sess.run(total_loss_1))\n",
    "\n",
    "loss_per_instance_2 = tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true)\n",
    "print (\"loss_per_instance_2=\",sess.run(loss_per_instance_2))\n",
    "# array([ 0.4790107 ,  1.19967598])\n",
    "\n",
    "total_loss_2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_hat, y_true))\n",
    "print (\"loss_total_2=\",sess.run(total_loss_2))\n",
    "# 0.83934333897877922\n",
    "\n",
    "\n",
    "total_loss_3 = tf.nn.sparse_softmax_cross_entropy_with_logits(y_hat, y_true)\n",
    "print (\"loss_total_3=\",sess.run(total_loss_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Operation\n",
    "\n",
    "```Python\n",
    "def train(total_loss, global_step):\n",
    "  \"\"\"Train CIFAR-10 model.\n",
    "\n",
    "  Create an optimizer and apply to all trainable variables. Add moving\n",
    "  average for all trainable variables.\n",
    "\n",
    "  Args:\n",
    "    total_loss: Total loss from loss().\n",
    "    global_step: Integer Variable counting the number of training steps\n",
    "      processed.\n",
    "  Returns:\n",
    "    train_op: op for training.\n",
    "  \"\"\"\n",
    "  # Variables that affect learning rate.\n",
    "  num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size\n",
    "  decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "\n",
    "  # Decay the learning rate exponentially based on the number of steps.\n",
    "  lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                  global_step,\n",
    "                                  decay_steps,\n",
    "                                  LEARNING_RATE_DECAY_FACTOR,\n",
    "                                  staircase=True)\n",
    "  tf.scalar_summary('learning_rate', lr)\n",
    "\n",
    "  # Generate moving averages of all losses and associated summaries.\n",
    "  loss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "  # Compute gradients.\n",
    "  with tf.control_dependencies([loss_averages_op]):\n",
    "    opt = tf.train.GradientDescentOptimizer(lr)\n",
    "    grads = opt.compute_gradients(total_loss)\n",
    "\n",
    "  # Apply gradients.\n",
    "  apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "  # Add histograms for trainable variables.\n",
    "  for var in tf.trainable_variables():\n",
    "    tf.histogram_summary(var.op.name, var)\n",
    "\n",
    "  # Add histograms for gradients.\n",
    "  for grad, var in grads:\n",
    "    if grad is not None:\n",
    "      tf.histogram_summary(var.op.name + '/gradients', grad)\n",
    "\n",
    "  # Track the moving averages of all trainable variables.\n",
    "  variable_averages = tf.train.ExponentialMovingAverage(\n",
    "      MOVING_AVERAGE_DECAY, global_step)\n",
    "  variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "  with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "    train_op = tf.no_op(name='train')\n",
    "\n",
    "  return train_op\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Training Step\n",
    "\n",
    "```Python\n",
    "def train():\n",
    "  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    images, labels = cifar10.distorted_inputs()\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.\n",
    "    logits = cifar10.inference(images)\n",
    "\n",
    "    # Calculate loss.\n",
    "    loss = cifar10.loss(logits, labels)\n",
    "\n",
    "    # Build a Graph that trains the model with one batch of examples and\n",
    "    # updates the model parameters.\n",
    "    train_op = cifar10.train(loss, global_step)\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    # Build the summary operation based on the TF collection of Summaries.\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "    # Build an initialization operation to run below.\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Start running operations on the Graph.\n",
    "    sess = tf.Session(config=tf.ConfigProto(\n",
    "        log_device_placement=FLAGS.log_device_placement))\n",
    "    sess.run(init)\n",
    "\n",
    "    # Start the queue runners.\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir, sess.graph)\n",
    "\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "      loss_value = sess.run([train_op, loss]) # Training\n",
    "     ï¼ƒ ...\n",
    "     # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
